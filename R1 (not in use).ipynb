{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddgeruajIgb5",
        "outputId": "2fae092b-2d90-449e-8eb4-83cefcecd9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Junko\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('WARNING') # https://stackoverflow.com/a/38873777\n",
        "\n",
        "from itertools import permutations\n",
        "import random\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTzARTmvIpPm"
      },
      "outputs": [],
      "source": [
        "pathToData = \"/content/drive/MyDrive/Colab Notebooks/data/plans\" #text files\n",
        "output = \"/content/drive/MyDrive/Colab Notebooks/data/out.txt\" #amalgamated text file\n",
        "csvPath = \"/content/drive/MyDrive/Colab Notebooks/data/data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3eHpGCQtpH_"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "os.makedirs(pathToData, exist_ok=True)\n",
        "#https://www.geeksforgeeks.org/reading-rows-from-a-csv-file-in-python/\n",
        "with open(csvPath, 'r') as csvFile:\n",
        "    reader = csv.reader(csvFile)\n",
        "    next(reader)\n",
        "    for index, row in enumerate(reader, start=1):\n",
        "        #https://www.w3schools.com/python/ref_string_strip.asp\n",
        "        addictionName = row[1].strip()\n",
        "        triggers = [row[2].strip().lower(), row[3].strip().lower(), row[4].strip().lower()]\n",
        "        warningSigns = [row[5].strip().lower(), row[6].strip().lower(), row[7].strip().lower()]\n",
        "        copingStrategies = [row[8].strip().lower(), row[9].strip().lower(), row[10].strip().lower()]\n",
        "        severity = float(row[11].strip())\n",
        "        #https://stackoverflow.com/questions/104420/how-do-i-generate-all-permutations-of-a-list\n",
        "        triggerPerm = list(permutations(triggers))\n",
        "        warningPerm = list(permutations(warningSigns))\n",
        "        strategyPerm = list(permutations(copingStrategies))\n",
        "        plan = []\n",
        "        for i in range(1, 11):\n",
        "            for trigger in triggerPerm:\n",
        "                for warning in warningPerm:\n",
        "                    for strategy in strategyPerm:\n",
        "                        plan.append(\"Addiction name: \" + addictionName + \"\\n\")\n",
        "                        plan.append(\"Triggers:\\n\")\n",
        "                        plan.append(\"- \" + trigger[0] + \"\\n\")\n",
        "                        plan.append(\"- \" + trigger[1] + \"\\n\")\n",
        "                        plan.append(\"- \" + trigger[2] + \"\\n\")\n",
        "                        plan.append(\"Addiction Severity:\\n\")\n",
        "                        plan.append(str(float(i / 10)) + \"\\n\")\n",
        "                        plan.append(\"Warning Signs:\\n\")\n",
        "                        plan.append(\"- \" + warning[0] + \"\\n\")\n",
        "                        plan.append(\"- \" + warning[1] + \"\\n\")\n",
        "                        plan.append(\"- \" + warning[2] + \"\\n\")\n",
        "                        plan.append(\"Coping Strategies:\\n\")\n",
        "                        plan.append(\"- \" + strategy[0] + \"\\n\")\n",
        "                        plan.append(\"- \" + strategy[1] + \"\\n\")\n",
        "                        plan.append(\"- \" + strategy[2] + \"\\n\")\n",
        "                        #print(plan)\n",
        "                        data.append(''.join(plan))\n",
        "                        plan = []\n",
        "        print(data)\n",
        "for i in range(len(data)):\n",
        "    formattedPlan = ''\n",
        "    currentPlan = data[i]\n",
        "    splitString = currentPlan.split(\"\\n\")\n",
        "    for i in range(len(splitString)):\n",
        "        formattedPlan = formattedPlan + str(splitString[i].strip())\n",
        "    addictionName = formattedPlan[0].replace('Addiction name: ', '')\n",
        "\n",
        "    file_name = f\"{addictionName.replace(' ', '_')}_plan_{index}{i}.txt\"\n",
        "    file_path = os.path.join(pathToData, file_name)\n",
        "\n",
        "    with open(file_path, 'w') as txt_file:\n",
        "        txt_file.write(formattedPlan.strip())\n",
        "\n",
        "print(pathToData)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a7bZdu7Nvtf"
      },
      "outputs": [],
      "source": [
        "#https://stackoverflow.com/questions/13613336/how-do-i-concatenate-text-files-in-python\n",
        "#https://www.geeksforgeeks.org/python-os-listdir-method/\n",
        "with open(output, 'w') as outfile:\n",
        "    for fname in os.listdir(pathToData):\n",
        "        with open(pathToData + \"/\" + fname) as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcj5NQhrJCOX"
      },
      "outputs": [],
      "source": [
        "with open(output, 'rb') as i: #https://stackoverflow.com/questions/15746954/what-is-the-difference-between-rb-and-rb-modes-in-file-objects\n",
        "    text = i.read().decode(encoding='utf-8') #https://www.tutorialspoint.com/python/string_decode.htm\n",
        "print(repr(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.geeksforgeeks.org/string-tensors-in-tensorflow/\n",
        "#https://www.tensorflow.org/api_docs/python/tf/strings/split\n",
        "def splitLines(plans):\n",
        "    return tf.strings.split(plans, sep=\"\\n\")"
      ],
      "metadata": {
        "id": "dlI7fSk0u-Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXgRkPDSMpTH"
      },
      "outputs": [],
      "source": [
        "#below code modified from Jeremie Wenger - Artifical Intelligence: https://drive.google.com/file/d/14WFNX1nfbdqhhg0nzr8aFzqdyIriYsa_/view\n",
        "text_vectorization = tf.keras.layers.TextVectorization( # note that we don't limit our vocab size here (no need)!\n",
        "    standardize=None,\n",
        "    split=splitLines, #https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "    output_mode=\"int\"\n",
        ")\n",
        "\n",
        "text_vectorization.adapt([text])\n",
        "\n",
        "TOKEN_INDEX = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "VOCAB_SIZE = len(text_vectorization.get_vocabulary())   # retrieve the vocab size afterwards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFJM0UyBPWHT",
        "outputId": "3c913e41-157f-4a2d-9c0b-273b1b132e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Addiction name: Gambling\\n  Triggers:\\n  - lots of money\\n  - raffles\\n  - lottery\\n  Addiction Severity:\\n  0.1\\n  Warning Signs:\\n  - wondering how to use excess cash\\n  - lingering around lottery stands\\n  - browsing raffle sites\\n  Coping Strategies:\\n  - block gambling sites\\n  - mindfulness\\n  - limit access to excess fundsAddiction name: Gambling\\n  Triggers:\\n  - lots of money\\n  - raffles\\n  - lottery\\n  Addiction Severity:\\n  0.1\\n  Warning Signs:\\n  - lingering around lottery stands\\n  - browsing raffle sites\\n  - wondering how to use excess cash\\n  Coping Strategies:\\n  - limit access to excess funds\\n  - mindfulness\\n  - block gambling sitesAddiction name: Gambling\\n  Triggers:\\n  - lottery\\n  - raffles\\n  - lots of money\\n  Addiction Severity:\\n  0.1\\n  Warning Signs:\\n  - wondering how to use excess cash\\n  - lingering around lottery stands\\n  - browsing raffle sites\\n  Coping Strategies:\\n  - limit access to excess funds\\n  - mindfulness\\n  - block gambling sitesAddiction name: Gambling\\n  Triggers:\\n  - lottery\\n'\n"
          ]
        }
      ],
      "source": [
        "lm_dataset_raw = tf.data.Dataset.from_tensor_slices([text])\n",
        "\n",
        "for t in lm_dataset_raw:\n",
        "    print(t.numpy()[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdPt-SIZPczN",
        "outputId": "574a3b02-2bf7-4a78-fa98-610aeba00042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([305   3  89 ... 189   7  23], shape=(167072,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "lm_dataset_tok = lm_dataset_raw.map(text_vectorization)\n",
        "\n",
        "for t in lm_dataset_tok:\n",
        "    print(t)\n",
        "    DATASET_LENGTH = t.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHpm2tDLPh3W",
        "outputId": "f2bb95a6-958f-44f6-fa15-74a27aef35a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(305, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# Flatten the dataset of one long tensor into a dataset of individual tokens (ChatGPT 4 12/23)\n",
        "lm_dataset_flat = lm_dataset_tok.flat_map(\n",
        "    lambda x: tf.data.Dataset.from_tensor_slices(x)\n",
        ")\n",
        "for t in lm_dataset_flat.take(2):\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AALNZyPYPkd6",
        "outputId": "a9837a49-ed20-4ca6-aee6-f2d8fefb27c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([305   3  89  76], shape=(4,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LENGTH = 3\n",
        "\n",
        "lm_dataset_seqs = lm_dataset_flat.batch(\n",
        "    SEQUENCE_LENGTH + 1,\n",
        "    drop_remainder=True\n",
        ")\n",
        "\n",
        "for t in lm_dataset_seqs.take(1):\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw3GKEE9Rzg5",
        "outputId": "73b43544-0229-422d-e2de-e77fac1b78c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 4)\n"
          ]
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "lm_dataset_batched = (\n",
        "    lm_dataset_seqs\n",
        "        .repeat()\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "for t in lm_dataset_batched.take(1):\n",
        "    print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JyFM3g1S2-2"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_dataset(tokens_batch):\n",
        "    x = tokens_batch[:, :-1]  # [a b c d e f g] the model predicts top to bottom,\n",
        "    y = tokens_batch[:, 1:]   # [b c d e f g h] a → b, a b → c, a b c → d, ..., in one go!\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = lm_dataset_batched.map(prepare_lm_dataset, num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-oXGKWXS-qA"
      },
      "outputs": [],
      "source": [
        "@tf.keras.utils.register_keras_serializable(\"positional_embedding\")\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    # new in Keras 3, see: https://keras.io/guides/making_new_layers_and_models_via_subclassing/#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known\n",
        "    # See also here: https://github.com/keras-team/keras-nlp/blob/4601d88a61a5d3d15279865769af5155804dd785/keras_nlp/src/layers/modeling/token_and_position_embedding.py#L115\n",
        "    def build(self, input_shape):\n",
        "        # token embeddings: semantic information\n",
        "        self.token_embeddings =tf.keras.layers.Embedding(\n",
        "            input_dim=self.input_dim, output_dim=self.output_dim\n",
        "        )\n",
        "        # position embeddings: syntactic (spatial/temporal) information\n",
        "        self.position_embeddings =tf.keras.layers.Embedding(\n",
        "            input_dim=self.sequence_length, output_dim=self.output_dim\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        positions = tf.range(start=0, limit=length, delta=1) # delta: step size\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        # both embeddings are simply added together!\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    # copied from the source here: https://github.com/keras-team/keras-nlp/blob/4601d88a61a5d3d15279865769af5155804dd785/keras_nlp/src/layers/modeling/token_and_position_embedding.py#L146\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return self.token_embeddings.compute_mask(inputs, mask=mask)\n",
        "\n",
        "    def get_config(self): # retrieve config as a dict\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giKzf4AAUCrG",
        "outputId": "044ea06a-f4b5-44e3-854c-db9c287321a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "tf.Tensor(\n",
            "[[27 43 25 28  1 47  1 12 11 44]\n",
            " [42 14 12 27 10 41 15 15 46 34]], shape=(2, 10), dtype=int32)\n",
            "\n",
            "i:\n",
            "[[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]\n",
            " [9]]\n",
            "\n",
            "j:\n",
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "\n",
            "Is i >= j? Boolean cast to ints. (Note the broadcasting)\n",
            "\n",
            "tf.Tensor(\n",
            "[[1 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 0]\n",
            " [1 1 1 1 1 1 1 1 1 1]], shape=(10, 10), dtype=int32)\n",
            "\n",
            "We want mask to have the same dims as input, using `tf.tile`.\n",
            "Creating the right multiplier for it:\n",
            "\n",
            "tf.Tensor([2 1 1], shape=(3,), dtype=int32)\n",
            "\n",
            "Final mask with batch dimensions:\n",
            "\n",
            "tf.Tensor(\n",
            "[[[1 0 0 0 0 0 0 0 0 0]\n",
            "  [1 1 0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1 0 0]\n",
            "  [1 1 1 1 1 1 1 1 1 0]\n",
            "  [1 1 1 1 1 1 1 1 1 1]]\n",
            "\n",
            " [[1 0 0 0 0 0 0 0 0 0]\n",
            "  [1 1 0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1 0 0]\n",
            "  [1 1 1 1 1 1 1 1 1 0]\n",
            "  [1 1 1 1 1 1 1 1 1 1]]], shape=(2, 10, 10), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "def get_causal_attention_mask(inputs):\n",
        "    print(\"Inputs:\")\n",
        "    print(inputs)\n",
        "    print()\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    print(f\"i:\\n{i}\")\n",
        "    print()\n",
        "    print(f\"j:\\n{j}\")\n",
        "    print()\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    print(\"Is i >= j? Boolean cast to ints. (Note the broadcasting)\")\n",
        "    print()\n",
        "    print(mask)\n",
        "    print()\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1])) # adding a batch dimension\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1),\n",
        "         tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    print(\"We want mask to have the same dims as input, using `tf.tile`.\")\n",
        "    print(\"Creating the right multiplier for it:\")\n",
        "    print()\n",
        "    print(mult)\n",
        "    print()\n",
        "    tile = tf.tile(mask, mult)\n",
        "    print(\"Final mask with batch dimensions:\")\n",
        "    print()\n",
        "    print(tile)\n",
        "    return tile\n",
        "\n",
        "mask = get_causal_attention_mask(tf.random.uniform(shape=(2,10), maxval=50, dtype=tf.int32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pePAWBGIUFcT"
      },
      "outputs": [],
      "source": [
        "@tf.keras.utils.register_keras_serializable(\"transformer_decoder\")\n",
        "class TransformerDecoder(tf.keras.layers.Layer):\n",
        "\n",
        "    # simplified class: we don't need two attention layers as we don't have data\n",
        "    # flowing from an encoder!\n",
        "\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim                              # parameters\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.supports_masking = True                            # MASK: enforcing causality\n",
        "\n",
        "    # new in Keras 3, see: https://keras.io/guides/making_new_layers_and_models_via_subclassing/#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known\n",
        "    def build(self, input_shape):\n",
        "        self.attention_1 = tf.keras.layers.MultiHeadAttention(  # multi-head attention\n",
        "            num_heads=self.num_heads, key_dim=self.embed_dim\n",
        "        )\n",
        "        self.dense_proj = tf.keras.Sequential(                  # dense layer on top: like a nonlinearity\n",
        "            [tf.keras.layers.Dense(self.dense_dim, activation=\"relu\"),\n",
        "             tf.keras.layers.Dense(self.embed_dim),\n",
        "             tf.keras.layers.Dropout(0.1)]\n",
        "        )\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization() # layer norm\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    # retrieve config as a dict (necessary for custom Keras layers)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "\n",
        "        # prepare the causal mask\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        # REGULAR MASKED ATTENTION\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask) # apply the causal mask\n",
        "\n",
        "        # residual / layer norm\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        # dense net (nonlinearity) / layer norm\n",
        "        proj_output = self.layernorm_2(self.dense_proj(attention_output_1))\n",
        "\n",
        "        # residual\n",
        "        return attention_output_1 + proj_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlIbhb-JUQxt",
        "outputId": "bbf06640-6477-4f3d-af85-c6b9be18330c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "EMBED_DIM = 256\n",
        "LATENT_DIM = 256\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 5\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "print(f\"learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "def build_model(embed_dim, latent_dim, num_heads, num_layers):\n",
        "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim)(inputs)\n",
        "    for i in range(num_layers):\n",
        "        x = TransformerDecoder(embed_dim, latent_dim, num_heads)(inputs=x) # no encoder input!\n",
        "    #x = tf.keras.layers.Dense(VOCAB_SIZE)(x)\n",
        "    outputs =tf.keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)    # probability distribution over the vocab\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.RMSprop(LEARNING_RATE),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model(EMBED_DIM, LATENT_DIM, NUM_HEADS, NUM_LAYERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "SV0nQsXfUT5Q",
        "outputId": "d425e31a-caba-48c0-c88b-319e1112870b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ positional_embedding_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m140,800\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m658,688\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m658,688\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m658,688\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m658,688\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m658,688\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m547\u001b[0m)           │         \u001b[38;5;34m140,579\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ positional_embedding_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">140,800</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_decoder_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">547</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">140,579</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,574,819\u001b[0m (13.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,574,819</span> (13.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,574,819\u001b[0m (13.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,574,819</span> (13.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaLx4554Ukx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009726d4-f4b9-4a99-c46b-fe0e67cb61bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models\n"
          ]
        }
      ],
      "source": [
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature                 # temperature reweighting\n",
        "    exp_preds = np.exp(predictions)                                 # these two lines are actually\n",
        "    predictions = exp_preds / np.sum(exp_preds)                     # a softmax\n",
        "    probas = np.random.multinomial(1, predictions, 1)               # sampling using our probabilities\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(tf.keras.callbacks.Callback):\n",
        "    def __init__(self,\n",
        "                 prompt,                                            # initial context\n",
        "                 generate_length,                                   # how many words to generate\n",
        "                 seq_length,\n",
        "                 temperatures=(1.,),                                # a range of different temperatures\n",
        "                 print_every=50):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.seq_length = seq_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_every = print_every\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch == 0 or (epoch + 1) % self.print_every == 0:\n",
        "            print()\n",
        "            print()\n",
        "            print(\"EPOCH\", epoch + 1)\n",
        "            print()\n",
        "            print(\"-\" * 40)\n",
        "            for temperature in self.temperatures:\n",
        "                msg = f\"temperature {temperature}\"\n",
        "                print(msg)\n",
        "                print(\"-\" * len(msg))\n",
        "                # TODO: write the optimised version that does not convert back and forth\n",
        "                #       from tokens to strings during generation (the current version could\n",
        "                #       however be used to print the string as it is being generated, with a\n",
        "                #       'typerwriter effect'...)\n",
        "                sentence = self.prompt                                                      # start with our prompt\n",
        "                for i in range(self.generate_length):\n",
        "                    tokenized_sentence = text_vectorization([sentence])                     # encode the sentence & feed to the model\n",
        "                    predictions = self.model(tokenized_sentence[:, - self.seq_length + 1:]) # which gives us predictions (crop to seq_len!)\n",
        "                    next_token = sample_next(predictions[0, -1, :])                         # use these to sample (get the index)\n",
        "                    sampled_token = TOKEN_INDEX[next_token]                                 # use the index to pick the token\n",
        "                    sentence += sampled_token + \"\\n\"                                            # add it to our sentence\n",
        "                print(sentence)\n",
        "                print()\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "# used when debugging\n",
        "class DatasetInspectionCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, dataset):\n",
        "        super(DatasetInspectionCallback, self).__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print()\n",
        "        print(f\"\\nStarting Epoch {epoch + 1}\")\n",
        "        for i, batch in enumerate(self.dataset.take(1)):\n",
        "            print(f\"Batch {i + 1}:\")\n",
        "            b1, b2 = batch\n",
        "            print(b1[0, ])\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "# Use the callback in model.fit()\n",
        "inspection_callback = DatasetInspectionCallback(lm_dataset)\n",
        "\n",
        "prompt = \"Gambling\\nTriggers:\\n- Boredom on weekends\\n- Being near a casino\\n- Financial stress\\nAddiction Severity:\\n0.9\\nWarning Signs:\\n\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=SEQUENCE_LENGTH,\n",
        "    seq_length=SEQUENCE_LENGTH,\n",
        "    temperatures=(0.5, 0.7, 1., 0.9), # a diverse range of temperature to see its effect\n",
        "    print_every=20,\n",
        ")\n",
        "\n",
        "MODELS_DIR = pathlib.Path(\"models\")\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    str(MODELS_DIR / \"tiny-gpt.keras\"),\n",
        "    monitor=\"loss\",\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "print(MODELS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIrmK8BYUqWv",
        "outputId": "36024191-cc13-40e4-fb0e-ded331101bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167072\n",
            "3\n",
            "41768\n",
            "64\n",
            "652\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "print(DATASET_LENGTH)\n",
        "print(SEQUENCE_LENGTH)\n",
        "print(DATASET_LENGTH // (SEQUENCE_LENGTH + 1))\n",
        "print(BATCH_SIZE)\n",
        "print(DATASET_LENGTH // (SEQUENCE_LENGTH + 1) // BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7YV0ImgZfZc"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflowjs\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "model.fit(\n",
        "    lm_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=DATASET_LENGTH // (SEQUENCE_LENGTH + 1) // BATCH_SIZE,\n",
        "    callbacks=[text_gen_callback, ckpt_callback]\n",
        ")\n",
        "tfjs.converters.save_keras_model(model, 'models/junko')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfzwl777ThXz"
      },
      "outputs": [],
      "source": [
        "#prompt = \"Gambling\\nTriggers:\\n- Boredom on weekends\\n- Being near a casino\\n- Financial stress\\nAddiction Severity:\\n0.9\\nWarning Signs:\\n- losing interest in hobbies\\n- tiredness\\n- biting nails\\nCoping Strategies:\\n\"\n",
        "#prompt = \"Addiction name: Gambling\\nTriggers:\\n- Boredom on weekends\\n- Being near a casino\\n- Financial stress\\nAddiction Severity:\\n0.9\\nWarning Signs:\\n\"\n",
        "prompt = \"Addiction name: Pornography\\nTriggers:\\n- loneliness\\n- boredom\\n- tiredness\\nAddiction Severity:\\n0.6\\nWarning Signs:\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVVivUL2Vh4W",
        "outputId": "b5db12aa-5970-466c-9daf-780e563fdbcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Addiction name: Pornography\n",
            "Triggers:\n",
            "- loneliness\n",
            "- boredom\n",
            "- tiredness\n",
            "Addiction Severity:\n",
            "0.6\n",
            "Warning Signs:\n",
            "- peer pressure\n",
            "- hiding transactions\n",
            "- secrecy\n",
            "Coping Strategies:\n",
            "Addiction Severity:\n",
            "Addiction Severity:\n",
            "Addiction Severity:\n",
            "Warning Signs:\n",
            "Warning Signs:\n",
            "  - focus on rebuilding and maintaining healthy relationshipsAddiction name: Gambling\n",
            "Coping Strategies:\n",
            "Addiction Severity:\n",
            "Addiction Severity:\n",
            "- realise the house always wins\n",
            "Warning Signs:\n",
            "Warning Signs:\n",
            "Warning Signs:\n",
            "Coping Strategies:\n",
            "Addiction Severity:\n",
            "Warning Signs:\n",
            "Warning Signs:\n",
            "- being enticed by free credits\n",
            "- having disposable income\n",
            "- being enticed by free credits\n",
            "Coping Strategies:\n",
            "- realise the house always wins\n",
            "Warning Signs:\n",
            "Addiction Severity:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "RELOAD = True\n",
        "if RELOAD:\n",
        "    model = tf.keras.models.load_model(MODELS_DIR / \"tiny-gpt.keras\") # to load a saved model\n",
        "\n",
        "def generate(sentence=\" \", generate_length=100, temperature=1.):\n",
        "    # TODO: write the optimised version that does not convert back and forth\n",
        "    #       from tokens to strings during generation (the current version could\n",
        "    #       however be used to print the string as it is being generated, with a\n",
        "    #       'typerwriter effect'...)\n",
        "    for i in range(generate_length):\n",
        "        tokenized_sentence = text_vectorization([sentence])                         # encode the sentence & feed to the model\n",
        "        predictions = model(tokenized_sentence[:, - SEQUENCE_LENGTH + 1:])          # which gives us predictions  (crop to seq_len!)\n",
        "        next_token = sample_next(predictions[0, -1, :], temperature)                # use these to sample (get the index)\n",
        "        sampled_token = TOKEN_INDEX[next_token]                                     # use the index to pick the token\n",
        "        sentence += sampled_token                                                   # add it to our sentence\n",
        "        sentence += os.linesep\n",
        "    print(sentence)\n",
        "\n",
        "def generate_warning_signs(sentence, generate_length, temperature):\n",
        "  #print(sentence)\n",
        "  empty = \"\"\n",
        "  for i in range(generate_length):\n",
        "    tokenized_sentence = text_vectorization([sentence])                         # encode the sentence & feed to the model\n",
        "    predictions = model(tokenized_sentence[:, - SEQUENCE_LENGTH + 1:])          # which gives us predictions  (crop to seq_len!)\n",
        "    next_token = sample_next(predictions[0, -1, :], temperature)                # use these to sample (get the index)\n",
        "    sampled_token = TOKEN_INDEX[next_token]                                     # use the index to pick the token\n",
        "    empty += sampled_token + \"\\n\"\n",
        "                                                 # add it to our sentence\n",
        "  #print(sentence)\n",
        "  return empty\n",
        "\n",
        "prompt += generate_warning_signs(prompt, 3, .9)\n",
        "prompt += \"Coping Strategies:\\n\"\n",
        "prompt += generate_warning_signs(prompt, 3, .9)\n",
        "#prompt += \"- \" + generate_warning_sign(sentence=prompt, generate_length=10, temperature=.9)\n",
        "#prompt += \"- \" + generate_warning_sign(sentence=prompt, generate_length=10, temperature=.9)\n",
        "#prompt += \"- \" + generate_warning_sign(sentence=prompt, generate_length=10, temperature=.9)\n",
        "print(prompt)\n",
        "\n",
        "model.save(\"models/tiny-gpt.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-4zCV97CT9zT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DA7kI4xb1ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d175751-6528-41a0-e603-dc4e344cd0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-24 14:12:43.037374: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740406363.070853    9740 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740406363.081166    9740 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[32m🌲 Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
            "failed to lookup keras version from the file,\n",
            "    this is likely a weight only file\n"
          ]
        }
      ],
      "source": [
        "!tensorflowjs_converter --input_format keras \\models/tiny-gpt.h5 \\models/junko"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "#https://medium.com/@andrew.w.sale/deploying-text-classification-from-keras-to-tensorflow-js-92c614dca4ec\n",
        "with open('TOKEN_INDEX.json', 'w') as file:\n",
        "  json.dump(TOKEN_INDEX, file)"
      ],
      "metadata": {
        "id": "IJA_9FmrMTz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GIjBaggzmN1i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}